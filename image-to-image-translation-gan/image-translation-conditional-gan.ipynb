{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including TensorFlow, Keras, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Dropout, Concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Dataset\n",
    "Load the dataset and preprocess the images for training, including resizing and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Dataset\n",
    "\n",
    "# Import additional required libraries\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Define function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, target_size=(256, 256)):\n",
    "    # Load image\n",
    "    image = load_img(image_path, target_size=target_size)\n",
    "    # Convert image to array\n",
    "    image = img_to_array(image)\n",
    "    # Normalize image to range [-1, 1]\n",
    "    image = (image / 127.5) - 1.0\n",
    "    return image\n",
    "\n",
    "# Define function to load dataset\n",
    "def load_dataset(dataset_path, target_size=(256, 256)):\n",
    "    images = []\n",
    "    for image_name in os.listdir(dataset_path):\n",
    "        image_path = os.path.join(dataset_path, image_name)\n",
    "        image = load_and_preprocess_image(image_path, target_size)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "# Example usage\n",
    "train_images = load_dataset('data/mini_pix2pix/train')\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(f'Training set shape: {train_images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Generator Model\n",
    "Define the architecture of the generator model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator Model\n",
    "def define_generator(image_shape):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=image_shape)\n",
    "\n",
    "    # Encoder\n",
    "    down1 = Conv2D(64, (4, 4), strides=(2, 2), padding='same')(inputs)\n",
    "    down1 = LeakyReLU(alpha=0.2)(down1)\n",
    "\n",
    "    down2 = Conv2D(128, (4, 4), strides=(2, 2), padding='same')(down1)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = LeakyReLU(alpha=0.2)(down2)\n",
    "\n",
    "    down3 = Conv2D(256, (4, 4), strides=(2, 2), padding='same')(down2)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = LeakyReLU(alpha=0.2)(down3)\n",
    "\n",
    "    down4 = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(down3)\n",
    "    down4 = BatchNormalization()(down4)\n",
    "    down4 = LeakyReLU(alpha=0.2)(down4)\n",
    "\n",
    "    down5 = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(down4)\n",
    "    down5 = BatchNormalization()(down5)\n",
    "    down5 = LeakyReLU(alpha=0.2)(down5)\n",
    "\n",
    "    down6 = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(down5)\n",
    "    down6 = BatchNormalization()(down6)\n",
    "    down6 = LeakyReLU(alpha=0.2)(down6)\n",
    "\n",
    "    down7 = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(down6)\n",
    "    down7 = BatchNormalization()(down7)\n",
    "    down7 = LeakyReLU(alpha=0.2)(down7)\n",
    "\n",
    "    down8 = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(down7)\n",
    "    down8 = BatchNormalization()(down8)\n",
    "    down8 = LeakyReLU(alpha=0.2)(down8)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = UpSampling2D(size=(2, 2))(down8)\n",
    "    up1 = Conv2D(512, (4, 4), padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = Dropout(0.5)(up1)\n",
    "    up1 = Concatenate()([up1, down7])\n",
    "\n",
    "    up2 = UpSampling2D(size=(2, 2))(up1)\n",
    "    up2 = Conv2D(512, (4, 4), padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = Dropout(0.5)(up2)\n",
    "    up2 = Concatenate()([up2, down6])\n",
    "\n",
    "    up3 = UpSampling2D(size=(2, 2))(up2)\n",
    "    up3 = Conv2D(512, (4, 4), padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = Dropout(0.5)(up3)\n",
    "    up3 = Concatenate()([up3, down5])\n",
    "\n",
    "    up4 = UpSampling2D(size=(2, 2))(up3)\n",
    "    up4 = Conv2D(512, (4, 4), padding='same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = Concatenate()([up4, down4])\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(up4)\n",
    "    up5 = Conv2D(256, (4, 4), padding='same')(up5)\n",
    "    up5 = BatchNormalization()(up5)\n",
    "    up5 = Concatenate()([up5, down3])\n",
    "\n",
    "    up6 = UpSampling2D(size=(2, 2))(up5)\n",
    "    up6 = Conv2D(128, (4, 4), padding='same')(up6)\n",
    "    up6 = BatchNormalization()(up6)\n",
    "    up6 = Concatenate()([up6, down2])\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    up7 = Conv2D(64, (4, 4), padding='same')(up7)\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    up7 = Concatenate()([up7, down1])\n",
    "\n",
    "    # Output layer\n",
    "    outputs = UpSampling2D(size=(2, 2))(up7)\n",
    "    outputs = Conv2D(3, (4, 4), padding='same', activation='tanh')(outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "image_shape = (256, 256, 3)\n",
    "generator = define_generator(image_shape)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Discriminator Model\n",
    "Define the architecture of the discriminator model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator Model\n",
    "def define_discriminator(image_shape):\n",
    "    # Input layer for real and generated images\n",
    "    input_image = Input(shape=image_shape)\n",
    "    target_image = Input(shape=image_shape)\n",
    "\n",
    "    # Concatenate the input and target images\n",
    "    merged = Concatenate()([input_image, target_image])\n",
    "\n",
    "    # First convolutional layer\n",
    "    d = Conv2D(64, (4, 4), strides=(2, 2), padding='same')(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    d = Conv2D(128, (4, 4), strides=(2, 2), padding='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # Third convolutional layer\n",
    "    d = Conv2D(256, (4, 4), strides=(2, 2), padding='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # Fourth convolutional layer\n",
    "    d = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # Fifth convolutional layer\n",
    "    d = Conv2D(512, (4, 4), strides=(2, 2), padding='same')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # Output layer\n",
    "    d = Conv2D(1, (4, 4), padding='same')(d)\n",
    "    patch_out = tf.keras.activations.sigmoid(d)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([input_image, target_image], patch_out)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "discriminator = define_discriminator(image_shape)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Conditional GAN Model\n",
    "Combine the generator and discriminator models to create the conditional GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional GAN Model\n",
    "def define_gan(generator, discriminator, image_shape):\n",
    "    # Make the discriminator not trainable when training the GAN\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Define the input for the generator\n",
    "    input_image = Input(shape=image_shape)\n",
    "\n",
    "    # Generate the target image using the generator\n",
    "    generated_image = generator(input_image)\n",
    "\n",
    "    # Get the discriminator's output for the generated image\n",
    "    gan_output = discriminator([input_image, generated_image])\n",
    "\n",
    "    # Define the GAN model\n",
    "    model = Model(input_image, [gan_output, generated_image])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "gan = define_gan(generator, discriminator, image_shape)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Models\n",
    "Compile the generator, discriminator, and conditional GAN models with appropriate loss functions and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Models\n",
    "\n",
    "# Compile the generator model\n",
    "generator.compile(loss='mae', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Compile the discriminator model\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "# Compile the conditional GAN model\n",
    "gan.compile(loss=['binary_crossentropy', 'mae'], optimizer=Adam(learning_rate=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Conditional GAN\n",
    "Train the conditional GAN model on the preprocessed dataset, including defining the training loop and updating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Conditional GAN\n",
    "\n",
    "# Define the training function\n",
    "def train_gan(generator, discriminator, gan, dataset, epochs=100, batch_size=1):\n",
    "    # Calculate the number of batches per epoch\n",
    "    batch_count = dataset.shape[0] // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batch_count):\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, dataset.shape[0], batch_size)\n",
    "            real_images = dataset[idx]\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_images = generator.predict(real_images)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            real_labels = np.ones((batch_size,) + discriminator.output_shape[1:])\n",
    "            fake_labels = np.zeros((batch_size,) + discriminator.output_shape[1:])\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch([real_images, real_images], real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch([real_images, fake_images], fake_labels)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train the generator (via the GAN model)\n",
    "            g_loss = gan.train_on_batch(real_images, [real_labels, real_images])\n",
    "\n",
    "            # Print the progress\n",
    "            print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch+1}/{batch_count}, D Loss: {d_loss[0]}, D Acc: {d_loss[1]}, G Loss: {g_loss[0]}')\n",
    "\n",
    "# Example usage\n",
    "train_gan(generator, discriminator, gan, train_images, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Visualize Translated Images\n",
    "Use the trained generator model to generate translated images and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Visualize Translated Images\n",
    "\n",
    "# Define function to denormalize images from range [-1, 1] to [0, 1]\n",
    "def denormalize_image(image):\n",
    "    return (image + 1.0) / 2.0\n",
    "\n",
    "val_images = load_dataset('data/mini_pix2pix/val')\n",
    "\n",
    "# Select a random sample of validation images\n",
    "num_samples = 5\n",
    "sample_indices = np.random.choice(val_images.shape[0], num_samples, replace=False)\n",
    "sample_images = val_images[sample_indices]\n",
    "\n",
    "# Generate translated images using the trained generator\n",
    "translated_images = generator.predict(sample_images)\n",
    "\n",
    "# Plot the original and translated images\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(num_samples):\n",
    "    # Original image\n",
    "    plt.subplot(2, num_samples, i + 1)\n",
    "    plt.imshow(denormalize_image(sample_images[i]))\n",
    "    plt.axis('off')\n",
    "    plt.title('Original')\n",
    "\n",
    "    # Translated image\n",
    "    plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "    plt.imshow(denormalize_image(translated_images[i]))\n",
    "    plt.axis('off')\n",
    "    plt.title('Translated')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
