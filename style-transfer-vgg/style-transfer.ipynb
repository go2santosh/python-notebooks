{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "\n",
    "Style transfer is a fascinating technique in image processing where the style of one image is applied to the content of another image. Essentially, it allows you to take the artistic style of a painting, for example, and apply it to a photograph, creating a new image that combines the content of the photograph with the style of the painting.\n",
    "\n",
    "The VGG-19 model is a convolutional neural network (CNN) that was developed by the Visual Geometry Group (VGG) at the University of Oxford. It is one of the most well-known and widely used deep learning models for image classification and feature extraction.\n",
    "\n",
    "The VGG-19 model is described in the research paper titled \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" by Karen Simonyan and Andrew Zisserman. This paper was published in 2014 and is available on arXiv (https://arxiv.org/abs/1409.1556).\n",
    "\n",
    "The seminal paper that introduced the concept of neural style transfer using the VGG-19 model is titled \"A Neural Algorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. This paper was published in 2015 and is available on arXiv1 (https://arxiv.org/abs/1508.06576).\n",
    "\n",
    "The VGG-19 model is popular in style transfer because of its ability to effectively capture and represent the visual features of images, making it ideal for combining the content of one image with the style of another.\n",
    "\n",
    "Here are some key points about the VGG-19 model:\n",
    "\n",
    "1. Architecture: VGG-19 consists of 19 layers, including 16 convolutional layers and 3 fully connected layers. The convolutional layers use small 3x3 filters, which helps in capturing fine details in images.\n",
    "\n",
    "2. Pre-trained Model: In the context of style transfer, the VGG-19 model is often used in its pre-trained form. This means it has already been trained on a large dataset (such as ImageNet) and has learned to extract useful features from images.\n",
    "\n",
    "3. Feature Extraction: For style transfer, the VGG-19 model is used to extract features from both the content and style images. These features are then used to compute the content and style losses, which guide the optimization process to generate the final stylized image.\n",
    "\n",
    "4. Layer Selection: Different layers of the VGG-19 model capture different levels of abstraction. Lower layers capture basic features like edges and textures, while higher layers capture more complex patterns. In style transfer, specific layers are chosen to compute the content and style representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device to use (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "def load_image(image_path, max_size=400, shape=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    if max_size:\n",
    "        size = max_size if max(image.size) > max_size else max(image.size)\n",
    "    if shape:\n",
    "        size = shape\n",
    "    \n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content and style images from data folder and send to the device for model inference\n",
    "content = load_image('data/santosh-sharma-content.jpg').to(device)\n",
    "style = load_image('data/radha-krishna-style.jpg', shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def displayImage(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "\n",
    "displayImage('data/santosh-sharma-content.jpg')\n",
    "displayImage('data/radha-krishna-style.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG19 model with pretrained weights and extract the feature layers (i.e. convolutional and pooling layers)\n",
    "vgg = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content and style loss functions\n",
    "def get_features(image, model, layers=None):\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', \n",
    "                  '19': 'conv4_1', '21': 'conv4_2', '28': 'conv5_1'}\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get content and style features\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gram matrices for each layer of our style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target image and prepare it for optimization\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each style layer\n",
    "style_weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.2, 'conv4_1': 0.2, 'conv5_1': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for content and style loss\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 5000  # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target image back to a PIL image and save it\n",
    "import numpy as np\n",
    "def im_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return Image.fromarray((image * 255).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1, steps+1):\n",
    "    target_features = get_features(target, vgg)\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        style_gram = style_grams[layer]\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        style_loss += layer_style_loss\n",
    "    \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if ii % 500 == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "\n",
    "        intermediate_image = im_convert(target)\n",
    "        intermediate_image.save('data/output_image_' + str(content_weight) + '_' + str(style_weight) + '_' + str(ii) + '.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image = im_convert(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image.save('data/output_image_final_' + str(content_weight) + '_' + str(style_weight) + '_' + str(ii) + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the output image\n",
    "displayImage('data/output_image.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
